\chapter{Introduction}
For almost 4 billion years, single celled organisms dominated the planet.  About 650 million years ago, the first multicellular animals emerged in the ocean.  340 million years ago, amphibians escaped the ocean and walked on land.  About 6 million years ago, humans split away from chimps.  10,000 years ago, after the ice caps of the Pleistocene retreated, we began to develop agriculture.  The efficiency compared to hunting and gathering allowed for some people to live off the surplus of production, and not be wholly concerned with the daily grind.  This freed the human mind to do other things.  About 600 years ago, the Gutenberg Press was invented, allowing for broad dissemination of information.  In 1946, a giant room sized computer called the ENIAC consumed 174 kilowatts to compute 5000 additions per second.  The latest iPhone today can compute 10 trillion operations per second in your pocket.  

There are a wide range of estimates for how many operations per second a human brain effectively computes, but we are already seeing superhuman performance emerging for many tasks.  In 1997 IBM's Deep Blue beat Garry Kasparov the world chess champion.  In 2016, neural nets could classify images of skin cancer better than highly trained dermatologists.  Regardless of your stance on general AI, the most powerful computer known in the universe, the human mind, is being challenged.  But it isn't really a challenge.  Eventually computers will outperform humans at every task.  Whether a computer has general intelligence will become a fuzzy question.  Pick a task, define a metric, gather enough data, apply graduate student descent, and eventually a neural net will achieve superhuman performance.  No laws of physics prevent this.  It's arrogant to think that the human mind is something so magical a computer can't surpass it.  The mind was developed to allow an organism to out-compete its neighbors at finding food and mates.  In the process, things like hunger, happiness, anger, memory, and wisdom, were all developed to maximize that organism's number of living descendants, while dealing with the obstacles on Earth.  I'd argue the human mind is actually a very narrow intelligence developed to deal with Earthly animal problems.  A silicon based intelligence could very well be more general than narrow human intelligence.

At its heart, modern AI is actually very simple.  There are inputs, outputs, labels, and knobs (parameters).  The knobs control how the neural net produces an output given its input.  In modern systems like GPT3, there are 175 billion knobs \cite{brown2020language}.  Training a neural net is the act of tuning the knobs until you are happy with the outputs given the inputs.  For example, imagine you want to train a neural net to predict whether an image is a cat or a dog.  The input would be an image.  To a neural net, an image is represented by a 2D array of pixels, and each pixel is represented by 3 numbers, corresponding to red, green, and blue intensities.  The output would be the probability that the picture is a cat (from which the probability that it is a dog can be inferred).  The knobs are initially in random positions.  When the first image is passed through the neural net, it will likely output 50\%, since it doesn't know anything yet.  But the picture is labelled as a cat.  The knobs must now be turned very slightly so that they would have output a higher percentage for cat, maybe 51\% cat.  How can the knobs be turned to achieve this?  Gradient descent via the chain rule of calculus, also known as backpropagation.  For each knob it can determined how much the probability assigned to cat would change given how much the knob is turned, keeping all other knobs the same. To train a neural net, pass these labeled inputs and turn the knobs until the outputs are correct.  It could be the case that you run out of data before the outputs are correct.  Then pass the dataset in again.  You can do this many times, but be careful because eventually the neural net will overfit to your training data, and it will not generalize well to new inputs.  Periodically evaluating performance on a validation set not used for training is a good way to determine if the neural net has overfit on the training data yet.

The same concept can be used to train a neural net that generates text.  Again, there are inputs, outputs, labels, and knobs.  The input would be a sequence of words or characters.  To a neural net this is represented as a vector of zeros with a one at the index of that word or character in the dictionary.  The output would be the next word in the sequence.  For example, the input could be "The man walks his" and the label "dog".

This simple concept of inputs, outputs, labels, and knobs tuned by gradient descent is at the heart of the modern AI revolution.

Marc Andreessen has famously claimed that software will eat the world.  Andrej Karpathy has argued that AI will eat software.  It follows then that AI will eat the world.  It has already begun to eat many problems in medicine.  Recently DeepMind showed significant progress in the elusive field of protein folding \cite{senior2020improved}.  From a raw genetic sequence of As, Ts, Cs, and Gs, AlphaFold can correctly predict the resulting protein structure on par with experimental procedures.  In fact it even predicted the correct structure of a particular protein that had been eluding researchers for 10 years.  If AI isn't the next phase of evolution, then genetically engineered organisms will be.  

An early example of AI applied to medicine was that of Professor Daphne Koller and her lab in 2011 \cite{beck2011systematic}.  They applied probabilistic graphical models to detect breast cancer in histology slides.  Her lab's work showed that researchers in the AI lab with a technical background outside of medicine could make contributions to medicine -- a field often thought of as a walled garden.  Since then a lot of progress has been made in automated breast cancer detection.  Google can now outperform radiologists at detecting breast cancer in x-ray images \cite{mckinney2020international}, and can assist pathologists looking at histology slides to increase their sensitivity from 83\% to 91\% \cite{steiner2018impact}.  

Another exciting line of work is that of detecting diabetic retinopathy from images taken with retinal cameras \cite{gulshan2016development}.  It is the fastest growing cause of blindness, with over 400 million diabetic patients worldwide.  If caught early, it can be treated.  Otherwise it leads to irreversible blindness.  Verily has deployed this technology in India for population screening.  This same line of work led to the discovery that from retinal images, a neural net could assess cardiovascular risk factors \cite{poplin2018prediction}.

AI has shown potential in detecting cancer from various data modalities.   Radiologist level performance was achieved with chest radiography interpretations for the detection of pneumothorax, nodule or mass, airspace opacity, and fracture \cite{majkowska2020chest}.  Pathologist level performance was achieved with Adenocarcinoma and squamous cell carcinoma, the most prevalent subtypes of lung cancer \cite{coudray2018classification}.

One major obstacle that AI + medicine has faced over the years is the lack of large datasets.  In fact this is the dilemma neural nets in general have faced until about 2012.  Neural nets have been around since at least the 80s, when Hinton invented backprop \cite{rumelhart1986learning}, Yann Lecun automated digit recognition for the post office \cite{lecun1989backpropagation}.  The rule of thumb was always that neural nets were the 3rd best solution.  Convex solutions like SVM often worked better.  Why have these non-convex neural nets had a resurgence all of a sudden?  Many people would credit it to Krizhevsky, dominating the image recognition challenge ILSVRC with AlexNet \cite{krizhevsky2012imagenet}.  I would credit it to Professor Fei-Fei Li and her group for creating ImageNet \cite{deng2009imagenet}.  Creating a large dataset was not considered a glamorous task.  Many researchers would rather invent a genius algorithm, not do the dirty work of creating a dataset.  But what we have seen again and again is that it's not even about the algorithm.  Little has changed from the 80s algorithmically.  What has changed is the amount of compute available, and the size of dataset available.  Andrew Ng has a nice plot that shows the performance of a neural net as dataset increases \cite{ng2016nuts}.  More data, more parameters, better performance.  We are seeing this in NLP with GPT3 \cite{brown2020language}.  GPT3 is trained with a very simple algorithm.  Give it a sequence of text and train it to predict the next word.  As it is fed more and more internet data, the text sequences it generates gets harder to distinguish from human generated.  After reading 45 terabytes of internet text and 314 zettaflops of training compute, GPT3 can write an essay that will convince you recycling is bad [CITE].  The more data we throw at AI, and the bigger the models grow, the better performance they achieve.

That said, there are some improvements being made to the algorithms.  For example, residual layers added great value to convolutional neural nets.  Very deep "highway" networks could be trained much more aggressively, overcoming the problem of vanishing gradients.  Recently, transformer networks have been outperforming both recurrent and convolutional neural nets at many tasks.  When I came to Stanford, the NLP and Computer Vision groups didn't have much in common other than that they were both in the same building.  After the ImageNet breakthrough, and Andrew Ng's 700+ person machine learning courses, many research groups started to use deep learning.  At least then there was a common framework with which to make progress.  Things like stochastic gradient descent algorithms, or ideas like data augmentation, could be shared between domains.  Recently, we are even seeing the same type of neural net architecture being shared between domains.  One net to rule them all.  So there is some value in improving the algorithms.  

But the most successful pursuits in AI are often by those who have the greatest compute and data resources available.  NVIDIA has played a big role in making more compute available to researchers.  But this isn't what caused the neural net resurgence.  It wasn't until Andrew Ng's student Adam Coates painstakingly wrote GPU kernels to train a neural net \cite{coates2013deep} that NVIDIA was even relevant.  No, it was ImageNet that has kicked off the latest AI boom.  Before ImageNet, there wasn't a dataset large enough to satiate a neural net's thirst.  Until ImageNet, you'd be better off extracting hand-crafted features in OpenCV than training a neural net.  After ImageNet, features could be learned from data.

It's clear that large ImageNet-like datasets are key to making progress in AI and medicine.  When I undertook the skin cancer project, no such dataset existed.  The greatest hurdle was creating the large dataset of skin lesion images.  This is why I have so much respect for ImageNet.  Because of the massive hype in AI, we are fortunately seeing many large scale organizations respond positively.  The UK has created a dataset, UK Biobank, comprising over half a million people \cite{sudlow2015uk}.  The data collected includes everything there is know about a patient: MRI scans, full genome sequence, blood tests, diagnoses, demographics, and much more.  There are two similar projects in the US underway.  One is run by Verily called Project Baseline.  The other is run by the US government called All of Us.  Large medical datasets like this will be essential to future progress in AI and health.

For a long time medicine has focused on helping the sick.  This is a noble endeavor, but can we prevent people from getting sick in the first place?  An ounce of prevention is worth a pound of cure.  We are constantly surrounded by advanced technology, let's put it to use.  When you go through the airport an expensive scanner sends harmless millimeter waves through your body to determine if you are carrying something dangerous.  But while they're at it, why not do a quick medical check?  Skin cancer can be detected from an image.  How often are people taking high resolution selfies or pictures with friends with modern smartphone cameras?  These photos could be processed for skin cancer.  A great deal of information is contained in stool and urine.  Can a smart toilet analyze this data?  Over 100 million people wear smartwatches that are capable of measuring electrocardiograms and photoplethysmograms.  From an ECG alone, atrial fibrillation can be detected.  What else can be detected from these smartwatches?  There is so much fertile ground in the area of preventive health and AI, ubiquitously monitoring you for disease.

A big issue in medicine in privacy.  Privacy is somewhat at odds with the requirements of AI.  AI requires a large centralized dataset.  Google faces a similar problem with many of their products.  Take email for example.  How can they train on user data without breaching their privacy?  One solution that has been gaining popularity is federated learning.  Essentially data stays on the user's device.  A parameter gradient is computed on-device and sent to the server to update the model.  Another approach is differential privacy.  This approach has nice theoretical properties but destroys a lot of the data.  AI and preventive care will require privacy respecting methods.

Another big issue with AI and medicine is regulation.  FDA approval can easily cost millions of dollars, in addition to lengthy clinical trials.  This prevents the classic silicon valley mantra of moving fast and breaking things.  And it should.  Startups shouldn't break people in the process of trying to make them healthier.  This dilemma has a lot in common with that of self driving cars.  How can automated driving get smarter without learning from mistakes?  Many brilliant people, perhaps most notably Sebastian Thrun, have made progress toward making this a reality.  Maybe some day, preventive healthcare algorithms and self driving car algorithms will have similar approaches.  Maybe there will be some kind of "safe" reinforcement learning.

I have focused on applying AI to two specific domains: classifying skin lesions from photos, and assessing risk for a broad range of conditions from biological waveforms such as electrocardiogram and photoplethsymogram.  A smartphone is capable of taking a photo of a skin lesion.  A smart watch is capable of measuring both an electrocardiogram and photoplethysmogram.

To approach the task of skin lesion classification, the first requirement was to create a dataset.  There was no ImageNet of skin cancer.  There are, however, many dermatology websites with semi-labeled images that doctors upload.  In all I found over 100k images online, creating the largest ever skin lesion image dataset known to AI.  It was during this time that I gained appreciation for ImageNet.  A lot of work went into creating this skin lesion dataset that a neural net could train on.  For example, the labels were high entropy strings.  The labels "malignant melanoma" and "melanoma, malignant" would be two totally different categories to a neural net.  The labels were streamlined with the help of simple heuristics and our dermatologist collaborators.  They helped categorize all the high entropy string labels into a nice taxonomy of diseases.  This allowed for later choosing the granularity desired to train with.  A coarse granularity would be benign vs malignant.  One could traverse the taxonomy further to get finer grained categories.  If the categories are too fine, not many images will be available for a given category.  There is a happy medium somewhere between the coarse benign vs malignant, and something too fine grained like basal cell carcinoma on left forearm vs basal cell carcinoma on right forearm.  In the former case, one would have many images for each category.  In the latter case, there may be 1 or 2 images in each category.  Another issue that came up was that many photos weren't actually photos of skin lesions.  Some were histology slides, others were computer generated figures.  To deal with this, I grabbed 100 images of manually verified skin lesions.  Then I grabbed 100 images of all the things I didn't want in the dataset.  I finetuned AlexNet to do binary classification on skin vs not skin.  It achieved really good accuracy on a held out test set so I let it loose on all the data.

I took a radically different approach for task of detecting problems in waveform data.  I created a dataset from scratch once and wasn't trying to do that again.  This time I found a nice dataset collected by highly motivated grad students at MIT.  They collected raw data from many different sources within a Harvard teaching hospital.  Further, not only was the data collected, the labels were already in a taxonomy.  The labels were ICD codes, used for billing.  These ICD codes are part of a taxonomy that contains all known diseases.  So I didn't have to create the dataset and I didn't have to create the taxonomy.  But for this project, there wasn't an ImageNet pretrained model like VGG or Inception that I could use.  I had to invent my own model architecture.  For one, the signals were 1 dimension instead of 2 dimensions as are images.  Also, there wasn't a strong precedent on how to train a neural net on biological waveforms.  For images, it is common practice to train on resolutions of about 256x256.  What length input should one use for waveforms, though?  Ultimately I settled on 2048 sample inputs, or about 16 seconds.

In this thesis I show results in applying AI to preventive healthcare in two domains.  The first domain is detecting skin cancer in images of skin lesions.  It is remarkable that harvesting raw internet images made it possible to achieve dermatologist level performance at classifying skin lesions.  If the raw data on the internet can be harnessed to match dermatologist performance, what other gold is waiting to be mined?  For one, there is GPT3.  

The second domain is in assessing risk for various conditions from biological waveforms measured in the hospital such as ECG and PPG.  Here it is remarkable that from such simple waveforms, a neural net is able to develop a sense of what organ isn't working properly.  For example, the neural net knows when something is wrong with the brain.  This is evident in the semantic inference heat map.  When a patient has a condition affecting their brain, ICD codes that affect the brain will carry high risk.  Similarly, if a patient has a condition affecting their liver, ICD codes that affect the liver will carry high risk.  This is remarkable because maybe predicting the exact ICD code isn't what is important.  A given ICD code has many synonyms, and it is semi random which one a doctor will choose.  Maybe assessing organ health is the thing that is actually tangible.
