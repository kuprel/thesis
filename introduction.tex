\chapter{Introduction}
For almost 4 billion years, single celled organisms dominated the planet.  About 650 million years ago, the first multicellular animals emerged in the ocean.  340 million years ago, amphibians escaped the ocean and walked on land.  About 6 million years ago, humans split away from chimps.  10,000 years ago, after the ice caps of the Pleistocene retreated, we began to develop agriculture.  The efficiency compared to hunting and gathering allowed for some people to live off the surplus of production, and not be wholly concerned with the daily grind.  This freed the human mind to do other things.  About 600 years ago, the Gutenberg Press was invented, allowing for broad dissemination of information.  In 1946, a giant room sized computer called the ENIAC consumed 174 kilowatts to compute 5000 additions per second.  The latest iPhone today can compute 10 trillion operations per second in your pocket.  

There are a wide range of estimates for how many operations per second a human brain effectively computes, but we are already seeing superhuman performance emerging for many tasks.  In 1997 IBM's Deep Blue beat Garry Kasparov the world chess champion.  In 2016, neural nets could classify images of skin cancer better than highly trained dermatologists.  Regardless of your stance on general AI, the most powerful computer known in the universe, the human mind, is being challenged.  But it isn't really a challenge.  Eventually computers will outperform humans at every task.  Whether a computer has general intelligence will become a fuzzy question.  Pick a task, define a metric, gather enough data, apply graduate student descent, and eventually a neural net will achieve superhuman performance.  No laws of physics prevent this.  It's arrogant to think that the human mind is something so magical a computer can't surpass it.  The mind was developed to allow an organism to out-compete its neighbors at finding food and mates.  In the process, things like hunger, happiness, anger, memory, and wisdom, were all developed to maximize that organism's number of living descendants, while dealing with the obstacles on Earth.  I'd argue the human mind is actually a very narrow intelligence developed to deal with Earthly animal problems.  A silicon based intelligence could very well be more general than narrow human intelligence.

Marc Andreessen has famously claimed that software will eat the world.  Andrej Karpathy has argued that AI will eat software.  It follows then that AI will eat the world.  It has already begun to eat many problems in medicine.  Recently DeepMind showed significant progress in the elusive field of protein folding \cite{senior2020improved}.  From a raw genetic sequence of As, Ts, Cs, and Gs, AlphaFold can correctly predict the resulting protein structure on par with experimental procedures.  In fact it even predicted the correct structure of a particular protein that had been eluding researchers for 10 years.  If AI isn't the next phase of evolution, then genetically engineered organisms will be.  

One early example of AI applied to medicine that inspired me was that of Professor Daphne Koller and her lab in 2011 \cite{beck2011systematic}.  They applied probabilistic graphical models to detect breast cancer in histology slides.  Her lab's work showed that researchers in the AI lab with a technical background outside of medicine could make contributions to medicine -- a field which I had previously thought of as a walled garden.  Since then a lot of progress has been made in automated breast cancer detection.  Google can now outperform radiologists at detecting breast cancer in x-ray images \cite{mckinney2020international}, and can assist pathologists looking at histology slides to increase their sensitivity from 83\% to 91\% \cite{steiner2018impact}.  

Another exciting line of work is that of detecting diabetic retinopathy from images taken with retinal cameras \cite{gulshan2016development}.  It is the fastest growing cause of blindness, with over 400 million diabetic patients worldwide.  If caught early, it can be treated.  Otherwise it leads to irreversible blindness.  Verily has deployed this technology in India for population screening.  This same line of work led to the discovery that from retinal images, a neural net could assess cardiovascular risk factors \cite{poplin2018prediction}.

AI has shown potential in detecting cancer from various data modalities.   Radiologist level performance was achieved with chest radiography interpretations for the detection of pneumothorax, nodule or mass, airspace opacity, and fracture \cite{majkowska2020chest}.  Pathologist level performance was achieved with Adenocarcinoma and squamous cell carcinoma, the most prevalent subtypes of lung cancer \cite{coudray2018classification}.

One major obstacle that AI + medicine has faced over the years is the lack of large datasets.  In fact this is the dilemma neural nets in general have faced until about 2012.  Neural nets have been around since at least the 80s, when Yann Lecun automated digit recognition for the post office \cite{lecun1989backpropagation}, and Hinton popularized backpropagation.  The rule of thumb was always that neural nets were the 3rd best solution.  Convex solutions like SVM often worked better.  Why have these non-convex neural nets had a resurgence all of a sudden?  Many people would credit it to Krizhevsky, dominating the image recognition challenge ILSVRC with AlexNet \cite{krizhevsky2012imagenet}.  I would credit it to Professor Fei-Fei Li and her group for creating ImageNet \cite{deng2009imagenet}.  Creating a large dataset was not considered a glamorous task.  Many researchers would rather invent a genius algorithm, not do the dirty work of creating a dataset.  But what we have seen again and again is that it's not even about the algorithm.  Little has changed from the 80s algorithmically.  What has changed is the amount of compute available, and the size of dataset available.  Andrew Ng has a nice plot that shows the performance of a neural net as dataset increases [CITE].  More data, more parameters, better performance.  We are seeing this in NLP with GPT3 [CITE].  GPT3 is trained with a very simple algorithm.  Give it a sequence of text and train it to predict the next word.  That's it.  As it is fed more and more of the internet's data, the text it generates gets harder to distinguish from human generated.  After reading [X] petabytes of internet data and training for [X] gigaflop days, GPT3 can write an essay that will convince you recycling is bad [CITE].  The more data we throw at AI, and the bigger the models grow, the better performance they achieve.

That said, there are some improvements being made to the algorithms.  For example, residual layers added great value to convolutional neural nets.  Very deep "highway" networks could be trained much more aggressively, overcoming the problem of vanishing gradients.  Recently, transformer networks have been outperforming both recurrent and convolutional neural nets at many tasks.  When I came to Stanford, the NLP and Computer Vision groups didn't have much in common other than that they were both in the same building.  After the ImageNet breakthrough, and Andrew Ng's 700+ person machine learning courses, many research groups started to use deep learning.  At least then there was a common framework with which to make progress.  Things like stochastic gradient descent algorithms, or ideas like data augmentation, could be shared between domains.  Recently, we are even seeing the same type of neural net architecture being shared between domains.  One net to rule them all.  So there is some value in improving the algorithms.  

But the most successful pursuits in AI are often by those who have the greatest compute and data resources available.  NVIDIA has played a big role in making more compute available to researchers.  But this isn't what caused the neural net resurgence.  It wasn't until Andrew Ng's student Adam Coates painstakingly wrote GPU kernels to train a neural net \cite{coates2013deep} that NVIDIA was even relevant.  No, it was ImageNet that has kicked off the latest AI boom.  Before ImageNet, there wasn't a dataset large enough to satiate a neural net's thirst.  Until ImageNet, you'd be better off extracting hand-crafted features in OpenCV than training a neural net.  After ImageNet, features could be learned from data.

It's clear that large ImageNet-like datasets are key to making progress in AI and medicine.  When I undertook the skin cancer project, no such dataset existed.  The greatest hurdle was creating the large dataset of skin lesion images.  This is why I have so much respect for ImageNet.  Because of the massive hype in AI, we are fortunately seeing many large scale organizations respond positively.  The UK has created a dataset, UK BioBank, comprising over half a million people \cite{sudlow2015uk}.  The data collected includes everything there is know about a patient: MRI scans, full genome sequence, blood tests, diagnoses, demographics, and much more.  There are two similar projects in the US underway.  One is run by Verily called Project Baseline.  The other is run by the US government called All of Us.  Large medical datasets like this will be essential to future progress in AI and health.

For a long time medicine has focused on helping the sick.  This is a noble endeavor, but can we prevent the sick?  An ounce of prevention is worth a pound of cure.  We are constantly surrounded by advanced technology, let's put it to use.  When you go through the airport an expensive scanner sends harmless millimeter waves through your body to determine if you are carrying something dangerous.  But while they're at it, why not do a quick medical check?  Skin cancer can be detected from an image.  How often are people taking high resolution selfies or pictures with friends with modern smartphone cameras?  These photos could be processed for skin cancer.  A great deal of information is contained in stool and urine.  Can a smart toilet analyze this data?  Over 100 million people wear smartwatches that are capable of measuring electrocardiograms and photoplethysmograms.  From an ECG alone, atrial fibrillation can be detected.  What else can be detected from these smartwatches?  There is so much fertile ground in the area of preventive health and AI, ubiquitously monitoring you for disease.

A big issue in medicine in privacy.  Privacy is somewhat at odds with the requirements of AI.  AI requires a large centralized dataset.  Google faces a similar problem with many of their products.  Take email for example.  How can they train on user data without breaching their privacy?  One solution that has been gaining popularity is federated learning [CITE].  Essentially data stays on the user's device.  A parameter gradient is computed on-device and sent to the server to update the model.  Another approach is differential privacy.  This approach has nice theoretical properties but destroys a lot of the data.  AI and preventive care will require privacy respecting methods.

Another big issue with AI and medicine is regulation.  FDA approval can easily cost millions of dollars, in addition to lengthy clinical trials.  This prevents the classic silicon valley mantra of moving fast and breaking things.  And it should.  We shouldn't break people in the process of trying to make them healthier.  This dilemma has a lot in common with that of self driving cars.  How can we automate driving without causing safety risks in the process?  Many brilliant people, most notably Sebastian Thrun, have made progress toward making this a reality.  Maybe some day, preventive healthcare algorithms and self driving car algorithms will have similar approaches.  Maybe some kind of "safe" reinforcement learning.

I have focused on applying AI to two specific domains: classifying skin lesions from photos, and assessing risk for a broad range of conditions from biological waveforms such as electrocardiogram and photoplethsymogram.  A smartphone is capable of taking a photo of a skin lesion.  A smart watch is capable of measuring both an electrocardiogram and photoplethysmogram.

To approach the task of skin lesion classification, the first requirement was to create a dataset.  There was no ImageNet of skin cancer.  There are, however, many dermatology websites with semi-labeled images that doctors upload.  In all I found over 100k images online, creating the largest ever skin lesion image dataset known to AI.  It was during this time that I gained appreciation for ImageNet.  A lot of work went into creating this skin lesion dataset that a neural net could train on.  For example, the labels were high entropy strings.  The labels "malignant melanoma" and "melanoma, malignant" would be two totally different categories to a neural net.  The labels were streamlined with the help of simple heuristics and our dermatologist collaborators.  They helped categorize all the high entropy string labels into a nice taxonomy of diseases.  This allowed for later choosing the granularity desired to train with.  A coarse granularity would be benign vs malignant.  One could traverse the taxonomy further to get finer grained categories.  If the categories are too fine, not many images will be available for a given category.  There is a happy medium somewhere between the coarse benign vs malignant, and something too fine grained like basal cell carcinoma on left forearm vs basal cell carcinoma on right forearm.  In the former case, one would have many images for each category.  In the latter case, there may be 1 or 2 images in each category.  Another issue that came up was that many photos weren't actually photos of skin lesions.  Some were histology slides, others were computer generated figures.  To deal with this, I grabbed 100 images of manually verified skin lesions.  Then I grabbed 100 images of all the things I didn't want in the dataset.  I finetuned AlexNet to do binary classification on skin vs not skin.  It achieved really good accuracy on a held out test set so I let it loose on all the data.

I took a radically different approach for task of detecting problems in waveform data.  I did the whole create-a-dataset thing once and wasn't trying to do that again.  This time I found a nice dataset collected by highly motivated grad students at MIT.  They collected raw data from a Harvard teaching hospital.  Further, not only was the data collected, the labels were already in a taxonomy.  The labels were ICD codes, used for billing.  These ICD codes are part of a taxonomy that contains all known diseases.  So I didn't have to create the dataset and I didn't have to create the taxonomy.  But for this project, there wasn't an Imagenet pretrained model of VGG or Inception that I could use.  I had to invent my own model architecture.  For one, the signals were 1 dimesion instead of 2 dimensions as are images.  Also, there wasn't a strong precedent on how to train a neural net on biological waveforms.  For images, it is common practice to train on resolutions of about 256x256.  What length input should one use for waveforms, though?  Ultimately I settled on 2048 sample inputs, or about 16 seconds.

In this thesis I show results in applying AI to preventive healthcare in two domains.  The first domain is detecting skin cancer in images of skin lesions.  It is remarkable that harvesting raw internet images made it possible to achieve dermatologist level performance at classifying skin lesions.  If the raw data on the internet can be harnessed to match dermatologist performance, what other gold is waiting to be mined?  For one, there is GPT3.  

The second domain is in assessing risk for various conditions from biological waveforms measured in the hospital such as ECG and PPG.  Here it is remarkable that from such simple waveforms, a neural net is able to develop a sense of what organ isn't working properly.  For example, the neural net knows when something is wrong with the brain.  This is evident in the semantic inference heat map.  When a patient has a condition affecting their brain, ICD codes that affect the brain will carry high risk.  Similarly, if a patient has a condition affecting their liver, ICD codes that affect the liver will carry high risk.  This is remarkable because maybe predicting the exact ICD code isn't what is important.  A given ICD code has many synonyms, and it is semi random which one a doctor will choose.  Maybe assessing organ health is the thing that is actually tangible.