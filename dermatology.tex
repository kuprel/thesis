\chapter{Dermatology}

\section{Introduction}
Skin cancer - the most common human malignancy \cite{society2016cancer, rogers2015incidence, stern2010prevalence} - is primarily diagnosed visually, beginning with an initial clinical screening, followed potentially by dermoscopic analysis, a biopsy, and histopathological examination. Automated classification of skin lesions using images is a challenging task due to the fine-grained variability of skin lesion appearance. Deep convolutional neural networks (CNN) \cite{lecun2015deep, lecun1998handbook} show great promise for general and highly variable tasks over many fine-grained object categories \cite{russakovsky2015imagenet, krizhevsky2012imagenet, ioffe2015batch,  szegedy2016rethinking, szegedy2015going, he2016deep}. Here we show classification of skin lesions using a single CNN, trained end-to-end directly from images using only their pixels and disease labels as inputs. We train a CNN on a dataset of 129,450 clinical images - two orders of magnitude larger than previous datasets \cite{masood2013computer} - consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical use cases: binary classification of (1) malignant carcinomas versus benign seborrheic keratoses, and (2) malignant melanomas versus benign nevi. Case (1) represents the identification of the most common cancer, and case (2) represents identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating, for the first time, an artificial intelligence with dermatologist-level skin cancer classification capability. It is projected that 6.3 billion smartphone subscriptions will exist by the year 2021 \cite{cerwall2015ericsson}. Outfitted with deep neural networks, mobile devices can extend the reach of dermatologists outside of the clinic, and enable low-cost universal access to vital diagnostic care. 

There are 5.4 million new cases of skin cancer each year in the United States \cite{rogers2015incidence}. One in five Americans will be diagnosed with a cutaneous malignancy in their lifetimes. While melanomas represent fewer than 5\% of all skin cancers in the United States, they account for approximately 75\% of all skin cancer-related deaths, and are responsible for over 10,000 deaths annually in the United States alone. Early detection is critical - the 5-year survival rate of melanoma drops from 97\% if detected in its earliest stages to 14\% if detected in its latest stages. The key contribution of this work is a computational method which we believe will allow medical practitioners and patients to proactively track skin lesion health and detect cancer early. By creating a novel disease taxonomy and partitioning algorithm which map individual diseases into training classes, we build a deep learning system for automated dermatology. 

Prior work in dermatological computer-aided classification \cite{masood2013computer, rosado2003accuracy, burroni2004melanoma} has lacked the generalization capability of medical practitioners due to insufficient data and a focus on standardized tasks such as dermoscopy \cite{kittler2002diagnostic, codella2015deep, codella2018skin} and histology image classification \cite{binder1998epiluminescence, altmeyer1997skin, clark1989model, schindewolf1993classification}. Dermoscopy images are acquired via a specialized instrument and histology images are acquired via invasive biopsy and microscopy; both of these modalities yield highly standardized images. Photographic images (e.g. smartphone images) exhibit variability in zoom, angle, lighting, etc, making classification significantly more challenging \cite{ramlakhan2011mobile, ballerini2013color}. We overcome this challenge using a data-driven approach - 1.41 million pre-training and training images make it robust to photographic variability. Many former techniques require extensive preprocessing, lesion segmentation, and extraction of domain-specific visual features prior to classification. In contrast, our system requires no hand-crafted features - it is trained end-to-end directly from image labels and raw pixels, with a single network for both photographic and dermoscopic images. The existing body of work uses small datasets of typically less than a thousand skin lesion images \cite{kittler2002diagnostic, codella2018skin, binder1998epiluminescence} that, as a result, do not generalize well to new images. We demonstrate generalizable classification with a new dermatologist-labeled dataset of 129,450 clinical images including 3,374 dermoscopy images. 

Deep learning algorithms, powered by advances in computation and extremely large datasets \cite{deng2009imagenet}, have recently been shown to exceed human performance at visual AI tasks such as Atari game playing \cite{mnih2015human}, strategic board games like Go \cite{silver2016mastering}, and object recognition \cite{russakovsky2015imagenet}. In this paper we outline the development of a deep convolutional neural network that matches the performance of dermatologists at three key diagnostic tasks: melanoma classification, melanoma classification using dermoscopy, and carcinoma classification.  We restrict comparison strictly to image-based classification.

\section{Methods}

We utilize a GoogleNet Inception-v3 CNN architecture9 pretrained on ~1.28 million images (1,000 object categories) from the 2014 ImageNet Large Scale Visual Recognition Challenge6, and train it on our dataset using transfer learning \cite{pan2009survey}. Figure 1 demonstrates the working system. The CNN is trained on general lesion classification using 757 disease classes. Our dataset is composed of dermatologist-labeled images organized in a novel tree-structured taxonomy of 2,032 diseases, where the individual diseases form the leaf nodes. The images come from 18 different clinician-curated, open-access online repositories as well as clinical data from the Stanford Hospital. Figure 2(a) shows a subset of the full taxonomy, which has been organized clinically and visually by medical experts. We split our dataset into 127,463 training/validation images,  and 1,942 biopsy-labeled test images.

To take advantage of fine-grained information contained in the structure of the taxonomy, we develop an algorithm (Extended Data Algorithm 1) to partition the diseases into fine-grained training classes (e.g. amelanotic melanoma and acrolentiginous melanoma). During inference, the CNN outputs a probability distribution over these fine classes. To recover the probabilities for coarser-level classes of interest (e.g. melanoma), we sum the probabilities of their descendants. See Methods and Extended Data Figure 1 for more details.

We validate the algorithm’s effectiveness in two ways, using nine-fold cross validation. First, we validate with a three-class disease partition - the level-1 nodes of the taxonomy - representing benign lesions, malignant lesions, and non-neoplastic lesions. Here the CNN achieves 72.1 ± 0.9\% overall accuracy (the average of individual inference class accuracies) and two dermatologists attain 65.56\% and 66.0\% accuracy on a subset of the validation set. Second, we validate with a nine-class disease partition - the level-2 nodes - such that the diseases of each class have similar medical treatment plans. The CNN achieves 55.4 ± 1.7\% overall accuracy whereas the same two dermatologists attain 53.3\% and 55.0\% accuracy. A CNN trained on a finer disease partition performs better than one trained directly on three or nine classes (see Extended Data Table 1), demonstrating the effectiveness of our partitioning algorithm. Since validation set images are labeled by dermatologists but not necessarily biopsy-proven, this metric is inconclusive, and instead serves to show that the CNN is learning relevant information.

For conclusiveness, we test our algorithm and dermatologists using strictly biopsy-proven images on medically significant use cases: distinguishing malignant versus benign (1) epidermal lesions (malignant carcinoma vs benign seborrheic keratosis) and (2) melanocytic lesions (malignant melanoma vs benign nevi). For (2), we display two trials, one using standard images and the other using dermoscopy images, which reflect two steps that a dermatologist might pursue to obtain a clinical impression. The same CNN is used across all three tasks. Figure 2(b) shows a few example images from each case, demonstrating the difficulty in distinguishing between malignant and benign lesions, which share many visual features. Our comparison metrics are sensitivity and specificity (SS) :

	sensitivity = TP P
	specificity = TN N

Where TP (true positives) is the number of correctly predicted malignant lesions, P is the number of malignant lesions shown, TN (true negatives) is the number of correctly predicted benign lesions, and N is the number of benign lesions shown. When a test set is fed through the CNN, it outputs a probability, p, of malignancy, per image. We can compute the sensitivity and specificity of these probabilities by choosing a threshold probability t such that the prediction y for each image is given by y = p > t. Varying t in the interval [0,1] generates a curve of sensitivities and specificities that the CNN can achieve.

Figure 3(a) shows a direct performance comparison between the CNN and over 81 board-certified dermatologists on epidermal and melanocytic lesion classification. For each image the dermatologists are asked whether to (a) biopsy/treat the lesion or (b) reassure the patient. Each red point on the plots represents the SS of a single dermatologist. The CNN outperforms any dermatologist whose SS point falls below the CNN’s blue curve - most do. The green points represents the average dermatologist (average SS of all red points), with error bars denoting one standard deviation. The area-under-the-curve (AUC) for each case is over 91\%. The data for this comparison (135 epidermal, 130 melanocytic, and 111 melanocytic-dermoscopy images) are sampled from the full test sets. Plotted in Figure 3(b) are the SS curves for our entire test set of biopsy-labeled images comprised of 707 epidermal, 225 melanocytic, and 1,010 melanocytic-dermoscopy images. From Figure 3(a) to Figure 3(b) we observe negligible changes in AUC (<0.03), validating the reliability of our results on a larger dataset. In a separate analysis with similar results (see Methods) dermatologists are asked if they believe a lesion is (a) malignant or (b) benign.

Using t-SNE \cite{van2008visualizing}, a technique for visualizing high-dimensional data, we examine in Figure 4 the features learned by the CNN that allow it to classify skin lesions accurately. Each point represents a skin lesion image (either epidermal or melanocytic) projected from the 2048-dimensional output of the CNN’s last hidden layer into two dimensions.   We see clusters of points of the same clinical classes - insets show thumbnail images of different diseases. Basal and squamous cell carcinomas are split across the malignant epidermal point cloud. Melanomas lie in the center, in contrast to nevi which lie on the right. Similarly, seborrheic keratoses lie across from their malignant counterparts. 

Here we demonstrate the effectiveness of deep learning in dermatology -  a technique which we apply to both general skin conditions and specific cancers. Using a single deep convolutional neural network trained at general skin lesion classification, we match the performance of over 21 tested dermatologists across three critical diagnostic tasks: carcinoma classification, melanoma classification, and melanoma classification under dermoscopy. We believe this fast, scalable method will be deployed on mobile devices, broadening the scope of primary care practice, and augmenting clinical decision-making for dermatology specialists. Further research is necessary to evaluate performance in a clinical setting, as a dermatologist’s clinical impression involves more than visual inspection of an isolated lesion. However, the ability to classify skin lesion images with the accuracy of a board-certified dermatologist has the potential to dramatically expand access to vital medical care. This method is primarily constrained by data and can classify many visual conditions if sufficient training examples exist. Deep learning is agnostic to the type of image data used and could be adapted to other specialties, including ophthalmology, otolaryngology, radiology, and pathology. 

\subsection{Datasets}
Our dataset comes from a combination of open-access dermatology repositories, the ISIC Dermoscopic Archive, the Edinburgh Dermofit Library, and data from the Stanford Hospital. The images from the online open-access dermatology repositories are annotated by dermatologists, not necessarily through biopsy. The ISIC Archive data used is composed strictly of melanocytic lesions that are biopsy-proven and annotated as malignant or benign. The Edinburgh Dermofit Library and data from the Stanford Hospital are biopsy-proven and annotated by individual disease names (e.g. actinic keratosis). 

\subsection{Taxonomy}
Our taxonomy represents 2,032 individual diseases arranged in a tree structure with its three root nodes representing general disease classes: (1) benign lesions, (2) malignant lesions, and (3) non-neoplastic lesions (Figure 2(b)). It was derived by dermatologists using a bottom-up procedure: individual diseases, initialized as leaf nodes, were merged based on clinical and visual similarity, until the entire structure was connected. This aspect of the taxonomy makes it useful in generating training classes that are both well-suited for machine learning classifiers and medically relevant. The root nodes are used in the first validation strategy and represent the most general partition. The children of the root nodes (i.e. malignant melanocytic lesions) are used in the second validation strategy, and represent disease classes that have similar clinical treatment plans.

\subsection{Data Preparation}
Blurry images and far-away images were removed from the test and validation sets, but still used in training. Our dataset contains sets of images corresponding to the same lesion but from multiple viewpoints, or multiple images of similar lesions on the same person. While this is useful training data, extensive care was taken to ensure that these sets were not split between the training and validation sets. Using image EXIF metadata, repository specific information, and nearest neighbor image retrieval with CNN features, we created an undirected graph connecting any pair of images that were determined to be similar. Connected components of this graph were not allowed to straddle the train/validation split and were randomly assigned to either train or validation. The test sets all came from independent, high-quality repositories of biopsy-proven images - the Stanford Hospital, the University of Edinburgh Dermofit Image Library, and the ISIC Dermoscopic Archive. No overlap (i.e. same lesion multiple viewpoints) exists between the test sets and the training/validation data.

\subsection{Sample Selection}
The epidermal, melanocytic, and melanocytic-dermoscopic tests of Figure 3(a) used 135 (65 malignant, 70 benign), 130 (33 malignant, 97 benign), and 111 (71 malignant, 40 benign) images, respectively. Their counterparts of Figure 3b used 707 (450 malignant, 257 benign), 225 (58 malignant, 167 benign), and 1010 (88 malignant, 922 benign) images, respectively. The number of images used for Figure 3(b) was based on the availability of biopsy-labeled data (i.e. malignant melanocytic lesions are exceedingly rare compared to benign melanocytic lesions). These numbers are statistically justified by the standards of the ILSVRC computer vision challenge6, which has 50-100 images per class for validation and test sets. For (a), 140 images were randomly selected from each set of (b), and a non-tested dermatologist removed any images of insufficient resolution (while the network accepts 299 x 299 image inputs, humans require larger images for clarity). 

\subsection{Disease Partitioning Algorithm}
The algorithm that partitions the individual diseases into training classes is outlined more formally in Extended Data Algorithm 1. It is a recursive algorithm, designed to leverage the taxonomy to generate training classes whose individual diseases are clinically and visually similar. The algorithm forces the average generated training class size to be slightly less than its only hyperparameter, maxClassSize. Together these components strike a balance between (1) generating training classes that are overly-fine grained and don’t have sufficient data to be learned properly, (2) generating training classes that are too coarse, too data abundant, and bias the algorithm towards them. With maxClassSize = 1000 this algorithm yields a disease partition of 757 classes. All training classes are descendants of inference classes.

\subsection{Training Algorithm}
We use Google’s Inception-v3 CNN architecture pre-trained to 93.33\% top-5 accuracy on the 1000 object classes (1.28M images) of the 2014 ImageNet Challenge following \cite{szegedy2016rethinking}. We then remove the final classification layer from the network and retrain it with our dataset, fine-tuning the parameters across all layers. During training we resize each image to 299 x 299 pixels in order to make it compatible with the original dimensions of the Inception-v3 network architecture and leverage the natural-image features learned by the ImageNet pretrained network. This procedure, known as transfer learning, is optimal given the amount of data available.

Our CNN is trained using backpropagation. All layers of the network are fine-tuned using the same global learning rate of 0.001 and a decay factor of 16 every 30 epochs. We use RMSProp with decay of 0.9, momentum of 0.9, and epsilon of 0.1. We use Google’s TensorFlow \cite{abadi2016tensorflow} deep learning framework to train, validate, and test our network. During training, images are augmented by a factor of 720. Each image is rotated randomly between 0 and 359 degrees. The largest upright inscribed rectangle is then cropped from the image, and is flipped vertically with a probability of 1/2. 

\subsection{Inference Algorithm}
We follow the convention that each node contains its children. Each training class is represented by a node in the taxonomy, and subsequently, all descendants. Each inference class is a node which has as its descendants some set of training nodes. An illustrative example is shown in Extended Data Figure 1, with red nodes as inference classes and green nodes as training classes. Given an input image, the CNN outputs a probability distribution over the training nodes. Probabilities over the taxonomy follow:

Where u is any node, P(u) is the probability of u, and C(u) are the child nodes of u. Thus to recover the probability of any inference node we simply sum the probabilities of its descendant training nodes. Note that in the validation strategies all training classes are summed into inference classes. However in the binary classification cases, the images in question are known to be either melanocytic or epidermal and so we utilize only the training classes which are descendants of either melanocytic or epidermal.

\subsection{Confusion Matrices}
Extended Data Figure 2 shows the confusion matrix of our method over the nine classes of the second validation strategy (Extended Data Table 1(d)) in comparison to the two tested dermatologists. This demonstrates the misclassification similarity between the CNN and human experts. Element (i, j) of each confusion matrix represents the empirical probability of predicting class j given that the ground truth was class i. Classes 7 and 8 - benign and malignant melanocytic lesions - are often confused for each other. Many images are mistaken as class 6, the inflammatory class, due to the high variability of diseases in this category. Note how easily malignant dermal tumors are confused for other classes, by both the CNN and dermatologists. They are essentially nodules under the skin that are challenging to visually diagnose. 

\subsection{Saliency Maps}
To visualize the pixels that a network is fixating on for its prediction, we generate saliency maps, shown in Extended Data Figure 3, for example images of the nine classes of Extended Data Table 1(d). Backpropagation is an application of the chain rule of calculus to compute loss gradients for all weights in the network. The loss gradient can also be backpropagated to the input data layer. By taking the L1 norm of this input layer loss gradient across the RGB channels, the resulting heat map intuitively represents the importance of each pixel for diagnosis. As can be seen, the network fixates most of its attention on the lesions themselves and ignores background and healthy skin.

\subsection{Sensitivity-Specificity Curves with different question}
In the main text we compare our CNN’s SS to that of over 21 dermatologists on the three diagnostic tasks of Figure 3. In that analysis each dermatologist was asked if they would (a) biopsy/treat the lesion, or (b) reassure the patient. This choice of question reflects the actual in-clinic task that dermatologists must perform - deciding whether or not to continue medically analyzing a lesion. A similar question to ask a dermatologist, though less clinically relevant, is if they believe a lesion is (a) malignant, or (b) benign. The results of this analysis are shown in Extended Data Figure 4. As in the main Figure 3, the CNN is on par with the performance of the dermatologists and outperforms the average. In the epidermal lesions test the CNN is just above one standard deviation above the average dermatologist, and in both melanocytic lesion tests the CNN is just below one standard deviation above the average dermatologist. 

\subsection{Use of human subjects}
All human subjects were board-certified dermatologists that consented to take our tests. This study was approved by the Stanford Institutional Review Board, under trial registration number 36050.

\section{Data Collection}
TODO

\section{Results}
TODO